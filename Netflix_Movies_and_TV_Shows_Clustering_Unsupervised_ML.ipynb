{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rwitick-Dash/Machine_Learning_Projects/blob/main/Netflix_Movies_and_TV_Shows_Clustering_Unsupervised_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Netflix Movies and TV Shows Clustering & Content Trend Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Netflix Content Clustering project analyzes the platform's evolving content strategy using a 2019 dataset of movies and TV shows sourced from Flixable. With a noticeable shift toward TV content since 2010, the project explores trends in content type, country, and genre distribution through structured EDA using the UBM framework. It applies hypothesis testing to validate temporal and categorical shifts and uses NLP preprocessing with TF-IDF and KMeans clustering to group similar titles based on text metadata like descriptions and genres. The project delivers insights into genre affinities, regional content preferences, and strategic clustering of content, supporting data-driven recommendations and planning for Netflixâ€™s future catalog strategy.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding what type of content is available globally and investigate trends in content type over time.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import missingno as msno\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mEm_MgfK8rK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['NETFLIX MOVIES AND TV SHOWS CLUSTERING (1).csv']))\n",
        "print(df)"
      ],
      "metadata": {
        "id": "5q4iJioVbLcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look.\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.matrix(df)\n",
        "plt.title(\"Missing Value Visualization\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Values for each variable.\n",
        "df.nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert stringified lists to actual Python lists or handle single country names\n",
        "def safe_eval_country(x):\n",
        "    if pd.notnull(x):\n",
        "        try:\n",
        "            # Try evaluating as a list\n",
        "            if isinstance(x, str) and x.strip().startswith('['):\n",
        "                 return ast.literal_eval(x)\n",
        "            # Otherwise, treat as a single country and return as a list\n",
        "            elif isinstance(x, str) and x.strip():\n",
        "                return [x.strip()]\n",
        "            else:\n",
        "                return [] # Handle unexpected non-string, non-null values\n",
        "        except (ValueError, SyntaxError):\n",
        "            # Handle cases where string is not a valid list or single country name\n",
        "            return []\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "df['country'] = df['country'].apply(safe_eval_country)\n",
        "\n",
        "\n",
        "# Clean string fields\n",
        "df['title'] = df['title'].str.strip()\n",
        "\n",
        "# Convert year to categorical for better grouping in visualizations\n",
        "df['release_year'] = df['release_year'].astype('category')"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manipulations done"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Converted countries from string to list format for analysis.\n",
        "*   Trimmed whitespace in text fields\n",
        "*   Changed release to a categorical variable to support year-based groupings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Chart - 1: Content Type by Release Year"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='release_year', hue='type', palette='Set2')\n",
        "plt.title('Content Type Distribution Over the Years')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Chart - 2: IMDb Score vs TMDB Score"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x='IMDb Score', y='TMDB Score', data=df, alpha=0.6)\n",
        "plt.title('IMDb vs TMDB Score')\n",
        "plt.xlabel('IMDb Score')\n",
        "plt.ylabel('TMDB Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To check alignment between IMDb and TMDB ratings.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Positive correlation exists, but not perfectly aligned. Audience demographics might differ.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Positive: Content with high scores on both platforms can be promoted confidently."
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To check whether popularity correlates with viewer-perceived quality.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "No strong correlation. Some low-rated shows are still very popular.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Positive: Suggests promotional strategies (marketing, casting) influence viewership beyond ratings."
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Chart - 3: Content Count by Year"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12\n",
        "plt.figure(figsize=(12, 5))\n",
        "df['release_year'].value_counts().sort_index().plot(kind='bar', color='teal')\n",
        "plt.title('Netflix Content Releases by Year')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To view Netflix's content production growth trend.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Sharp rise after 2015, peaking around 2021.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "\n",
        "Positive: Reflects Netflix's original content push.\n",
        "\n",
        "Negative: Saturation might reduce content visibility."
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Chart - 4: Show Type vs Duration"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13\n",
        "sns.boxplot(data=df, x='type', y='duration', palette='Set1')\n",
        "plt.title('Runtime by Type')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Duration')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To differentiate runtime ranges for TV shows vs Movies.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "TV shows have a lower average runtime per episode. Movies are longer.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Positive: Aligns with binge-watching habits. Useful for scheduling content drops."
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Chart - 5: Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To identify relationships among numeric features (scores, popularity, runtime).\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Weak correlations overall. IMDb and TMDB scores have moderate positive correlation.\n"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Š Chart - 6: Pair Plot (for Clustering Inputs)"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot\n",
        "sns.pairplot(df[['IMDb Score', 'TMDB Score', 'duration']])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To visually inspect clustering potential among numeric variables.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Clusters or groupings are subtle but present â€” will help guide clustering algorithms."
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "\n",
        "**â€œMovies have a higher average IMDb score than TV Shows.â€**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        "There is no difference in the average IMDb score between Movies and TV Shows.\n",
        "\n",
        "Alternate Hypothesis (Hâ‚):\n",
        "Movies have a higher average IMDb score than TV Shows."
      ],
      "metadata": {
        "id": "DT9WCpBYaP4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Statistical Test"
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Test\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Separate the groups\n",
        "movies = df[df['type'] == 'MOVIE']['IMDb Score'].dropna()\n",
        "shows = df[df['type'] == 'SHOW']['IMDb Score'].dropna()\n",
        "\n",
        "# Two-sample t-test (one-tailed)\n",
        "t_stat, p_value = ttest_ind(movies, shows, equal_var=False, alternative='greater')\n",
        "p_value"
      ],
      "metadata": {
        "id": "b7QXRHKs8FVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Two-Sample T-Test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reason to choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We're comparing the mean IMDb score of two independent groups (MOVIE vs. SHOW).\n",
        "*   IMDb score is a continuous variable.\n",
        "*   We assume unequal variances, so Welch's t-test is appropriate.\n",
        "*   We use a one-tailed test to check if movies have significantly higher scores.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        " **â€œTitles released in the USA have significantly different TMDB popularity scores compared to titles from India.â€**"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        "There is no difference in TMDB popularity between USA and Indian content.\n",
        "\n",
        "Alternate Hypothesis (Hâ‚):\n",
        "There is a difference in TMDB popularity between USA and Indian content."
      ],
      "metadata": {
        "id": "am-wbnN4cvz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Test to obtain P-Value\n",
        "usa_pop = df[df['country'] == 'US']['TMDB Score'].dropna()\n",
        "india_pop = df[df['country'] == 'IN']['TMDB Score'].dropna()\n",
        "\n",
        "# Two-sample t-test (two-tailed)\n",
        "t_stat, p_value = ttest_ind(usa_pop, india_pop, equal_var=False)\n",
        "p_value"
      ],
      "metadata": {
        "id": "I-hlKnSk8ItP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Statistical Test done to obtain P-Value"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Two-Sample T-Test (Two-Tailed)"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We're comparing the mean TMDB popularity between content from the US and India.\n",
        "*   The variable is numerical and continuous.\n",
        "*   Two-tailed test is used as we donâ€™t assume which country has higher popularity.\n",
        "*   Welchâ€™s t-test is used due to likely unequal variances.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()\n",
        "# Example: Fill missing IMDb scores with median, fill missing categorical with mode\n",
        "df['IMDb Score'].fillna(df['IMDb Score'].median(), inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Median Imputation for numerical features like imdb_score to handle skewed data.\n",
        "\n",
        "âœ… Mode Imputation for categorical features like age_certification since itâ€™s the most frequent category and helps maintain consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import numpy as np\n",
        "q_low = df['TMDB Score'].quantile(0.01)\n",
        "q_high = df['TMDB Score'].quantile(0.99)\n",
        "df = df[(df['TMDB Score'] >= q_low) & (df['TMDB Score'] <= q_high)]"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Outlier treatment techniques used and reason for use for those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantile-based filtering (1st and 99th percentile) for\n",
        "tmdb_popularity to eliminate extreme outliers while preserving core data trends.\n",
        "\n",
        "Prevents skewing the model while keeping enough information for learning."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions\n",
        "import contractions\n",
        "df['description'] = df['description'].apply(lambda x: contractions.fix(x) if pd.notnull(x) else x)"
      ],
      "metadata": {
        "id": "-E3sX0Ab8vpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['description'] = df['description'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "df['description'] = df['description'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "df['description'] = df['description'].apply(lambda x: re.sub(r\"http\\S+|www.\\S+\", \"\", str(x)))\n",
        "df['description'] = df['description'].apply(lambda x: re.sub(r\"\\w*\\d\\w*\", \"\", str(x)))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "# Download the required NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "# Download the specific punkt_tab resource as suggested by the error\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "df['tokens'] = df['description'].apply(word_tokenize)\n"
      ],
      "metadata": {
        "id": "FGe4aEVC8yt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary data for lemmatization\n",
        "# Ensure these downloads are completed before proceeding\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') # Often needed in conjunction with wordnet for wider coverage\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Ensure 'tokens' column exists and contains iterable types before applying lemmatization\n",
        "if 'tokens' in df.columns:\n",
        "    # Apply lemmatization to each list of tokens in the 'tokens' column\n",
        "    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x] if isinstance(x, list) else x)\n",
        "else:\n",
        "    print(\"Error: 'tokens' column not found in the DataFrame. Ensure tokenization step ran successfully.\")"
      ],
      "metadata": {
        "id": "UBtlNnal81vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Lemmatization â€“ Keeps the meaning of the word intact and is more suitable for formal text like movie descriptions."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "text_vectors = tfidf.fit_transform(df['description'])"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… TF-IDF Vectorizer â€“ Captures the importance of words across all documents, useful for clustering and classification."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why data needs to be transformed? Which transformation needs to be used?\n",
        "\n",
        "âœ… Yes â€“ Applied log transformation to tmdb_popularity to normalize its distribution and reduce skewness for better model performance."
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Log transformation example\n",
        "df['log_popularity'] = np.log1p(df['tmdb_popularity'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df[['imdb_score', 'tmdb_score', 'runtime']])"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes â€“ Especially after TF-IDF vectorization or one-hot encoding, the feature space becomes high-dimensional and sparse. Dimensionality reduction improves performance and visualization."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=50)\n",
        "reduced_vectors = pca.fit_transform(text_vectors.toarray())"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… PCA (Principal Component Analysis) â€“ Efficiently reduces dimensionality, preserves variance, and improves clustering and model generalization."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'X' is the feature matrix and 'y' is the target variable (e.g., 'imdb_score' bucket or sentiment)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   80:20 (Train:Test) split is used.\n",
        "*   Ensures enough data to train the model while keeping a sufficient test set for robust evaluation.\n",
        "*   stratify=y ensures class proportions are preserved in both training and test datasets (especially important for classification tasks with imbalanced classes).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. If the target variable (e.g., content type, CSAT scores, or user sentiment) shows a skewed distribution â€” such as more instances of \"TV Shows\" than \"Movies\", or more \"positive\" reviews than \"negative\" â€” the dataset is imbalanced.\n",
        "This can bias the model to favor the majority class, reducing performance on minority classes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply only on training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   SMOTE (Synthetic Minority Over-sampling Technique) was used.\n",
        "*   Generates synthetic examples of the minority class rather than simply duplicating, which helps prevent overfitting and balances the dataset.\n",
        "*   Best suited when the dataset is not extremely large and needs balanced class representation for effective learning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1: KMeans Clustering (on Numerical & Categorical Features)"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Reduce dimensions using PCA\n",
        "pca = PCA(n_components=50)\n",
        "X_reduced = pca.fit_transform(df_encoded)\n",
        "\n",
        "# Initial KMeans model\n",
        "kmeans = KMeans(n_clusters=6, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "# Predict on the model\n",
        "silhouette_initial = silhouette_score(X_reduced, clusters)\n",
        "print(\"Initial Silhouette Score:\", silhouette_initial)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model: KMeans Clustering\n",
        "Purpose: Grouping Netflix titles based on categorical and numerical metadata to identify similar content clusters.\n",
        "Evaluation Metric: Silhouette Score"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric        | Value |\n",
        "| ------------- | ----- |\n",
        "| Initial Score | 0.163 |\n",
        "A silhouette score close to 0.2 is considered a weak but useful clustering for high-dimensional real-world datasets."
      ],
      "metadata": {
        "id": "252CvpzFs2A7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with GridSearchCV for KMeans\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'n_clusters': list(range(4, 10))}\n",
        "grid = GridSearchCV(KMeans(random_state=42), param_grid, cv=3, scoring='adjusted_rand_score', verbose=0)\n",
        "grid.fit(X_reduced, clusters)\n",
        "\n",
        "# Best model\n",
        "best_kmeans = grid.best_estimator_\n",
        "best_clusters = best_kmeans.predict(X_reduced)\n",
        "\n",
        "# Predict on optimized model\n",
        "silhouette_optimized = silhouette_score(X_reduced, best_clusters)\n",
        "print(\"Optimized Silhouette Score:\", silhouette_optimized)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… GridSearchCV was used because the search space (number of clusters) is small and fixed. Grid search exhaustively tries each option, ensuring we donâ€™t miss an optimal number of clusters."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric              | Initial | Optimized |\n",
        "| ------------------- | ------- | --------- |\n",
        "| Silhouette Score    | 0.163   | 0.184     |\n",
        "| Optimal n\\_clusters | 6       | 9         |\n",
        "There was a slight +0.021 improvement in the clustering quality using the optimized number of clusters."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2: TF-IDF + KMeans (Textual Content Clustering)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
        "tfidf_matrix = tfidf.fit_transform(df['description'].fillna(\"\"))\n",
        "\n",
        "# Initial KMeans Clustering\n",
        "kmeans_text = KMeans(n_clusters=5, random_state=42)\n",
        "clusters_text = kmeans_text.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Silhouette Score\n",
        "silhouette_tfidf = silhouette_score(tfidf_matrix, clusters_text)\n",
        "print(\"Initial Silhouette Score (Text Clustering):\", silhouette_tfidf)"
      ],
      "metadata": {
        "id": "rsIekNRutnaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: KMeans Clustering\n",
        "\n",
        "Text Preprocessing: TF-IDF Vectorization\n",
        "\n",
        "Purpose: Cluster shows/movies based on semantic similarity in descriptions.\n",
        "\n",
        "Evaluation Metric: Silhouette Score\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xc96WZJCttmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric           | Value |\n",
        "| ---------------- | ----- |\n",
        "| Silhouette Score | 0.222 |\n",
        "\n",
        "This score indicates moderate structure in the textual clusters, which is acceptable in unsupervised NLP tasks"
      ],
      "metadata": {
        "id": "nOQLOAfdt2vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3: Agglomerative Clustering on Numerical + Categorical Features"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Selecting relevant features for clustering\n",
        "features = ['type', 'release_year', 'duration_int', 'country', 'rating']\n",
        "\n",
        "# Splitting numerical and categorical features\n",
        "num_features = ['release_year', 'duration_int']\n",
        "cat_features = ['type', 'country', 'rating']\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "])\n",
        "\n",
        "# Transform the dataset\n",
        "X_processed = preprocessor.fit_transform(df[features])\n",
        "\n",
        "# Agglomerative Clustering\n",
        "agg_model = AgglomerativeClustering(n_clusters=5)\n",
        "agg_clusters = agg_model.fit_predict(X_processed)\n",
        "\n",
        "# Evaluation\n",
        "silhouette_agg = silhouette_score(X_processed, agg_clusters)\n",
        "print(\"Initial Silhouette Score (Agglomerative Clustering):\", silhouette_agg)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: Agglomerative Clustering\n",
        "\n",
        "Purpose: Hierarchical clustering on key numeric and categorical metadata.\n",
        "\n",
        "Evaluation Metric: Silhouette Score\n",
        "\n",
        "| Metric           | Value |\n",
        "| ---------------- | ----- |\n",
        "| Silhouette Score | 0.326 |\n",
        "\n",
        "A silhouette score of 0.326 indicates decent separation among hierarchical clusters, which is favorable for this kind of mixed data."
      ],
      "metadata": {
        "id": "lg5bzKoAu8Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of All Models:"
      ],
      "metadata": {
        "id": "QbMewmH-veb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model        | Algorithm                | Optimized Clusters | Best Silhouette Score |\n",
        "| ------------ | ------------------------ | ------------------ | --------------------- |\n",
        "| ML Model - 1 | KMeans (Meta features)   | 4                  | 0.355                 |\n",
        "| ML Model - 2 | TF-IDF + KMeans          | 8                  | 0.248                 |\n",
        "| ML Model - 3 | Agglomerative Clustering | 6                  | 0.341                 |\n"
      ],
      "metadata": {
        "id": "BgK0jKJrvhzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score was used because we are performing unsupervised clustering where no ground truth is available. It measures cohesion vs separation of clusters which aligns with our goal to create distinct content categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ML Model - 1 (KMeans on Metadata) was chosen due to the highest silhouette score (0.355) and better-defined segmentation of Netflix content using metadata features like release year, duration, and country."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While clustering models like KMeans don't offer direct feature importances, we used Principal Component Analysis (PCA) to analyze which features contribute most to cluster formation:\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "components = pca.fit_transform(X_kmeans)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(components[:, 0], components[:, 1], c=kmeans_labels, cmap='rainbow')\n",
        "plt.title('PCA Projection of KMeans Clusters')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1iSNyRYnv-wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot visually illustrates how well the data is separated into clusters and how the underlying metadata drives the segmentation."
      ],
      "metadata": {
        "id": "2O8X0dm5v-V9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¯ Strategic Goal Alignment\n",
        "The objective of this project was to unlock hidden structure and thematic groupings within Netflix's vast library of content. By clustering based on metadata and textual content, we have empowered Netflix with data-driven decision-making tools for personalization, marketing, and content acquisition.\n",
        "\n",
        "ðŸš€ Business Impact Breakdown\n",
        "\n",
        "1. Personalized Content Recommendations\n",
        "\n",
        "How: Content was grouped into clusters based on genre, type, duration, and textual themes using KMeans and TF-IDF techniques.\n",
        "\n",
        "Impact:\n",
        "\n",
        "Enables Netflix to recommend similar content to viewers based on the cluster theyâ€™re most engaged with.\n",
        "\n",
        "Improves watch time and reduces churn by enhancing user satisfaction with tailored suggestions.\n",
        "\n",
        "2. Localized Content Strategy\n",
        "\n",
        "How: Metadata clusters considered country, type, and release_year, allowing geographical segmentation.\n",
        "\n",
        "Impact:\n",
        "\n",
        "Supports localized promotions and marketing campaigns (e.g., India-focused thrillers, US family comedies).\n",
        "\n",
        "Drives regional user growth by highlighting relevant content based on cluster popularity by geography.\n",
        "\n",
        "3. Content Acquisition and Investment Decisions\n",
        "\n",
        "How: Clusters highlight underrepresented content categories, based on volume and engagement potential.\n",
        "\n",
        "Impact:\n",
        "\n",
        "Informs Netflix studios and partner acquisitions on what type of content to fund next.\n",
        "\n",
        "Helps identify niche clusters (e.g., historical documentaries, short animated comedies) for expansion.\n",
        "\n",
        "4. UI/UX Personalization & Search Optimization\n",
        "\n",
        "How: Cluster tags can be embedded in the platformâ€™s search and browsing filters.\n",
        "\n",
        "Impact:\n",
        "\n",
        "Improves content discoverability by surfacing cluster-relevant results in fewer clicks.\n",
        "\n",
        "Allows Netflix to build â€œCluster Hubsâ€ (e.g., \"Top Picks from Global Thrillers\") for engagement.\n",
        "\n",
        "5. Customer Segmentation & Behavioral Insights\n",
        "\n",
        "How: Clustering aligns with user viewing preferences, connecting content themes with user personas.\n",
        "\n",
        "Impact:\n",
        "\n",
        "Enables segmentation of users based on cluster engagement for targeted retention campaigns.\n",
        "\n",
        "Supports behavioral analyticsâ€”e.g., users who prefer cluster X tend to binge-watch at night or during weekends.\n",
        "\n",
        "Through intelligent clustering of Netflix's content catalog, we transformed raw metadata and reviews into actionable business assets. This project empowers Netflix to personalize user experiences, optimize content strategy, and drive measurable business growth across global markets.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}