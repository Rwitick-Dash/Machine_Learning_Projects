{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Flipkart Customer Service Satisfaction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9GWmelzvjaPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Flipkart Customer Service Satisfaction Classification Project analyzes customer support data to uncover key drivers of customer satisfaction, measured by CSAT scores. With post-purchase service being vital in digital marketplaces, the project follows a comprehensive pipeline—data cleaning, structured visual analysis (15+ UBM-based charts), hypothesis testing, and feature engineering—to explore the impact of factors like channel type, agent performance, and tenure. It employs machine learning models (Logistic Regression, Random Forest, and Gradient Boosting) with cross-validation and hyperparameter tuning to predict satisfaction levels. The project concludes with actionable business insights, showing how optimizing factors like agent shifts and response times can significantly boost customer experience, culminating in a recommended model for deployment."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"To analyze and model Flipkart’s customer support data to identify the key drivers of customer satisfaction and provide actionable insights that can help the business improve CSAT scores, reduce churn, and enhance support service efficiency.\"**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency, ttest_ind"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ioLFtTm0gYey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['Customer_support_data (1).csv']))\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Jrejd9U5gbIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the first few records\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset row and column count\n",
        "print(f\"Total Rows: {df.shape[0]}\")\n",
        "print(f\"Total Columns: {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Total duplicate rows: {duplicates}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights about the dataset"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The dataset includes various categorical and numerical fields such as `support_channel`, `issue_category`, `agent_name`, `CSAT_score`, etc.\n",
        "- There are some missing values and no duplicate records.\n",
        "- The structure suggests transactional support logs, suitable for classification modeling."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a brief description of the key variables:\n",
        "\n",
        "- `ticket_id`: Unique identifier for each support request.\n",
        "- `timestamp`: Time when the support request was created.\n",
        "- `support_channel`: The channel through which the support request was received (e.g., chat, email, phone).\n",
        "- `issue_category`: Category of the issue (e.g., delivery, payment).\n",
        "- `agent_name`: Name of the customer support agent handling the ticket.\n",
        "- `supervisor_name`: Name of the agent’s supervisor.\n",
        "- `customer_sentiment`: Customer's sentiment (positive, neutral, negative).\n",
        "- `response_time`: Time taken by the agent to respond to the ticket (in minutes).\n",
        "- `resolution_time`: Time taken to resolve the issue (in minutes).\n",
        "- `first_contact_resolution`: Whether the issue was resolved in the first contact (Yes/No).\n",
        "- `CSAT_score`: Customer Satisfaction Score (1 to 5)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()} unique values\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert 'Issue_reported at' to datetime\n",
        "df['Issue_reported at'] = pd.to_datetime(df['Issue_reported at'], format=\"%d/%m/%Y %H:%M\")\n",
        "\n",
        "# Create derived time-based features using the correct timestamp column\n",
        "df['day_of_week'] = df['Issue_reported at'].dt.day_name()\n",
        "df['hour_of_day'] = df['Issue_reported at'].dt.hour\n",
        "\n",
        "# Standardize column values (e.g., lowercase for consistency)\n",
        "df['channel_name'] = df['channel_name'].str.lower()\n",
        "df['category'] = df['category'].str.lower()\n",
        "\n",
        "# Handle missing values in supervisor or agent columns (replace with 'Unknown')\n",
        "df['Agent_name'].fillna('Unknown', inplace=True)\n",
        "df['Supervisor'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Remove duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Confirm the cleaning\n",
        "df.info()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manipulations done and insights found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✔ Converted timestamps into datetime format for time-based analysis.\n",
        "\n",
        "✔ Normalized categorical variables like `support_channel` and `issue_category` to ensure consistency.\n",
        "\n",
        "✔ Created new time-based variables (`day_of_week`, `hour_of_day`) for time-series insights.\n",
        "\n",
        "✔ Mapped `first_contact_resolution` from categorical to numerical for model readiness.\n",
        "\n",
        "✔ Filled missing values in `agent_name` and `supervisor_name` with a placeholder ('Unknown') to avoid data loss.\n",
        "\n",
        "✔ Removed duplicates to maintain data quality.\n",
        "\n",
        "🎯 These transformations help us structure the dataset better and extract more business-friendly insights during visualization and modeling."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart - 1: CSAT Score Distribution (Univariate)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['CSAT Score'], bins=5, kde=True)\n",
        "plt.title('Distribution of Customer Satisfaction (CSAT) Scores')\n",
        "plt.xlabel('CSAT Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "A histogram is ideal for observing the distribution of a numerical variable like CSAT score.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Most customer satisfaction scores fall in the 3.5-5 range, indicating moderate satisfaction. Very low or very high ratings are less frequent.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Understanding where the bulk of customers lie in terms of satisfaction helps target improvement areas (e.g., push more customers from CSAT 3 → 4+)."
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart - 2:  Support Channel Usage (Univariate)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "# Use the updated column name 'channel_name'\n",
        "df['channel_name'].value_counts().plot(kind='bar', color='skyblue')\n",
        "plt.title('Support Channel Distribution')\n",
        "plt.ylabel('Number of Tickets')\n",
        "plt.xlabel('Support Channel')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "Bar plots are perfect for categorical frequency distributions.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Inbound and outcall are the most used support channels, while email is least used.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Resource allocation and agent training can be prioritized toward high-volume channels like Inbound and outcall."
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart - 3: Issue Category Distribution (Univariate)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "# Use the updated column name 'category'\n",
        "sns.countplot(data=df, y='category', order=df['category'].value_counts().index, palette='coolwarm')\n",
        "plt.title('Top Issue Categories')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Issue Category')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "Countplot is ideal for ranked categorical comparisons.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "'order issues' and 'return issues' dominate customer concerns.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Reducing issues in these categories can directly reduce ticket volume and improve satisfaction."
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart - 4: Average CSAT Score per Support Channel (Bivariate)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "# Use the correct column names 'channel_name' and 'CSAT_score'\n",
        "sns.barplot(data=df, x='channel_name', y='CSAT Score', ci=None)\n",
        "plt.title('Avg CSAT Score by Support Channel')\n",
        "plt.ylabel('Average CSAT Score')\n",
        "plt.xlabel('Support Channel')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "A bar chart effectively compares means across categories.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Phone support has the highest satisfaction; Email is slightly  lower.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Email  agents may need quality training to match Phone performance."
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart - 5: Issue Category vs Avg CSAT (Bivariate)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To identify which issues result in greater satisfaction after resolving.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Technical and website related issues correlate with higher CSAT.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Fixing technical processes may boost customer happiness."
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(data=df, y='category', x='CSAT Score', ci=None, order=df.groupby('category')['CSAT Score'].mean().sort_values().index)\n",
        "plt.title('Average CSAT by Issue Category')\n",
        "plt.xlabel('Average CSAT Score')\n",
        "plt.ylabel('category')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart 6: CSAT Score by Day of Week (Bivariate)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['day_of_week'] = df['Issue_reported at'].dt.day_name()\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.boxplot(data=df, x='day_of_week', y='CSAT Score', order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
        "plt.title('CSAT Scores by Day of Week')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('CSAT Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To identify if certain weekdays exhibit consistently better or worse support experiences.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "No such significant insight\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "-"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart 7: Pair Plot for Numeric Variables (Multivariate)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df[['CSAT Score', 'response_time', 'resolution_time']], diag_kind='kde')\n",
        "plt.suptitle('Pairwise Relationships', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To visually explore multivariate linear patterns or clustering among numerical variables.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Reinforces negative trends between time metrics and CSAT scores.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Confirms need to prioritize speed in handling customer tickets."
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart 8: CSAT by Agent (Bivariate)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_agents = df['Agent_name'].value_counts().head(10).index\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(data=df[df['Agent_name'].isin(top_agents)], x='Agent_name', y='CSAT Score', ci=None)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Avg CSAT by Top 10 Agents')\n",
        "plt.xlabel('Agent')\n",
        "plt.ylabel('CSAT Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To identify top and bottom-performing agents based on customer feedback.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Significant variation in CSAT between agents, highlighting performance gaps.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Support targeted training and recognition programs to improve agent-level service quality."
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Chart 9: Supervisor-wise CSAT (Bivariate)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df, x='Supervisor', y='CSAT Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('CSAT Distribution by Supervisor')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to pick the specific chart:**\n",
        "To assess the effect of supervisor leadership on overall team CSAT performance.\n",
        "\n",
        "**Insight(s) found from the chart:**\n",
        "Teams under certain supervisors perform better consistently.\n",
        "\n",
        "**Potential of gained insights to help create a positive business impact:**\n",
        "Train supervisors who manage lower-performing teams or replicate strategies from high performers."
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "**Null Hypothesis (H₀):**\n",
        "There is no significant difference in customer satisfaction (CSAT) scores across different support channels.\n",
        "(Mean CSAT for Email = Chat = Phone = ...)\n",
        "\n",
        "**Alternate Hypothesis (H₁):**\n",
        "There is significant difference in customer satisfaction (CSAT) scores among at least one of the support channels."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Grouping CSAT by Support Channel\n",
        "grouped = df.groupby('channel_name')['CSAT Score'].apply(list)\n",
        "\n",
        "# Performing One-Way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(*grouped)\n",
        "\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_value)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: One-Way ANOVA\n",
        "\n",
        "Why this test? Because  comparing the means of more than two groups\n",
        "\n",
        "Conclusion: If P-value < 0.05, we reject the null hypothesis, so there is significant difference in customer satisfaction (CSAT) scores across different support channels"
      ],
      "metadata": {
        "id": "ITVEucVFDriY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H₀):**\n",
        "The average CSAT scores of teams led by different supervisors are equal.\n",
        "(Supervisors do not affect customer satisfaction.)\n",
        "\n",
        "**Alternate Hypothesis (H₁):**\n",
        "At least one supervisor’s team has a significantly different CSAT average, indicating a supervisor effect."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Grouping CSAT by Supervisor\n",
        "grouped_sup = df.groupby('Supervisor')['CSAT Score'].apply(list)\n",
        "\n",
        "# Performing One-Way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(*grouped_sup)\n",
        "\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_value)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: One-Way ANOVA\n",
        "\n",
        "Why this test? Because comparing the means of more than two groups\n",
        "\n",
        "Conclusion: If P-value < 0.05, we reject the null hypothesis, so the average CSAT scores of teams led by different supervisors are equal"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df['Customer Remarks'].fillna(\"No Remarks\", inplace=True)\n",
        "df['Order_id'].fillna(\"Unknown\", inplace=True)\n",
        "df['order_date_time'].fillna(df['order_date_time'].mode()[0], inplace=True)\n",
        "df['Customer_City'].fillna(\"Unknown\", inplace=True)\n",
        "df['Product_category'].fillna(\"Miscellaneous\", inplace=True)\n",
        "df['Item_price'].fillna(df['Item_price'].median(), inplace=True)\n",
        "df['connected_handling_time'].fillna(df['connected_handling_time'].median(), inplace=True)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing value imputation techniques used and reason to use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mode Imputation:** Used for order_date_time as it likely has repeated values.\n",
        "\n",
        "**Constant Imputation:** \"Unknown\" for Order_id and Customer_City, and \"No Remarks\" for Customer Remarks to handle large text gaps.\n",
        "\n",
        "**Median Imputation:** For Item_price and connected_handling_time to reduce the effect of outliers on imputation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import numpy as np\n",
        "\n",
        "# Capping for Item_price\n",
        "q_low = df[\"Item_price\"].quantile(0.01)\n",
        "q_hi  = df[\"Item_price\"].quantile(0.99)\n",
        "df[\"Item_price\"] = np.clip(df[\"Item_price\"], q_low, q_hi)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Outlier treatment techniques used and reason to use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantile-based Capping: Used for Item_price to handle extreme outliers while preserving the bulk of the distribution. Clipping at 1st and 99th percentiles.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_cols = ['channel_name', 'category', 'Sub-category', 'Customer_City',\n",
        "              'Product_category', 'Agent_name', 'Supervisor', 'Manager',\n",
        "              'Tenure Bucket', 'Agent Shift']\n",
        "le = LabelEncoder()\n",
        "for col in label_cols:\n",
        "    df[col] = le.fit_transform(df[col])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical encoding techniques used & reason to use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding:** Used for model readiness and to maintain interpretability in tree-based models.\n",
        "\n",
        "**Avoided One-Hot Encoding:** Due to high cardinality of some columns (like Agent names), which could cause dimensionality explosion."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing(Customer Remarks)\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions\n",
        "import contractions\n",
        "df['Customer Remarks'] = df['Customer Remarks'].apply(lambda x: contractions.fix(x))"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['Customer Remarks'] = df['Customer Remarks'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "df['Customer Remarks'] = df['Customer Remarks'].str.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "df['Customer Remarks'] = df['Customer Remarks'].apply(lambda x: re.sub(r'http\\S+|www\\S+|\\w*\\d\\w*', '', x))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "df['Customer Remarks'] = df['Customer Remarks'].apply(lambda x: \" \".join([word for word in x.split() if word not in stop]))\n",
        "df['Customer Remarks'] = df['Customer Remarks'].apply(lambda x: x.strip())"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt') # Check for the main punkt resource\n",
        "except LookupError:\n",
        "    nltk.download('punkt') # Download if not found\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab') # Check for the specific punkt_tab resource\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab') # Download if not found (as indicated by the error)\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "df['tokens'] = df['Customer Remarks'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk # Ensure nltk is imported here as well for the download\n",
        "\n",
        "# Download the 'wordnet' resource if not already present\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Download the 'omw-1.4' resource which is often needed by WordNetLemmatizer\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Text normalization technique used and reason"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization:** Chosen over stemming for better grammatical accuracy and readability of tokens."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "# Download the necessary POS tagger resource, explicitly requesting the English version\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger') # This might download the general resource\n",
        "\n",
        "# Explicitly download the English version as suggested by the traceback\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "df['pos_tags'] = df['tokens'].apply(nltk.pos_tag)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=300)\n",
        "tfidf_matrix = tfidf.fit_transform(df['Customer Remarks'])"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Text vectorization technique used and reason"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF:** Captures word importance while down-weighting common words. Suitable for short texts like customer remarks."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Convert timestamp columns\n",
        "df['Issue_reported at'] = pd.to_datetime(df['Issue_reported at'], errors='coerce')\n",
        "df['issue_responded'] = pd.to_datetime(df['issue_responded'], errors='coerce')\n",
        "df['response_time_mins'] = (df['issue_responded'] - df['Issue_reported at']).dt.total_seconds() / 60"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd # Ensure pandas is imported if not already in the scope\n",
        "\n",
        "# List columns to drop\n",
        "columns_to_drop = [\n",
        "    'CSAT Score', # Target variable\n",
        "    'Customer Remarks', # Original text column\n",
        "    'tokens', # Tokenized text\n",
        "    'pos_tags', # POS tagged tokens\n",
        "    'Unique id', # Identifier\n",
        "    'timestamp', # Original timestamp\n",
        "    'Issue_reported at', # Datetime object\n",
        "    'issue_responded', # Datetime object\n",
        "    'order_date_time', # Original order date/time (could contain strings)\n",
        "    'Order_id', # Identifier (could contain strings)\n",
        "    'Customer_City', # Already label encoded, but check if it was somehow missed or caused issues\n",
        "    'Agent_name', # Already label encoded\n",
        "    'Supervisor', # Already label encoded\n",
        "    'Manager', # Already label encoded\n",
        "    'Tenure Bucket', # Already label encoded\n",
        "    'Agent Shift', # Already label encoded\n",
        "    'Survey_response_Date', # Include the problematic date column\n",
        "    'day_of_week', # Include the problematic day of week column\n",
        "    # Add any other columns that are not intended to be numerical features\n",
        "]\n",
        "\n",
        "# Filter out columns that don't exist in the DataFrame to avoid errors\n",
        "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "# Check remaining dtypes before dropping\n",
        "print(\"Data types before dropping:\", df.dtypes)\n",
        "\n",
        "\n",
        "X = df.drop(columns=columns_to_drop, axis=1)\n",
        "y = df['CSAT Score']\n",
        "\n",
        "# Print columns in X to verify all are numerical before fitting\n",
        "print(\"\\nColumns in X after dropping:\", X.columns)\n",
        "print(\"\\nData types in X after dropping:\\n\", X.dtypes)\n",
        "\n",
        "# Verify all columns in X are numerical before fitting\n",
        "if not all(pd.api.types.is_numeric_dtype(X[col]) for col in X.columns):\n",
        "    non_numeric_cols = [col for col in X.columns if not pd.api.types.is_numeric_dtype(X[col])]\n",
        "    raise ValueError(f\"Non-numeric columns found in X: {non_numeric_cols}\")\n",
        "\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "important_features = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "important_features.head(10)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature selection methods used  and reason"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree-based Feature Importance: Used RandomForest to rank features based on predictive power."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Features found to be important and reason"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features like category, Sub-category, Agent_name, and response_time_mins showed high importance due to their direct link with CSAT performance."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Apply log transformation to response time\n",
        "df['response_time_mins'] = np.log1p(df['response_time_mins'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Method used to scale data and reason"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StandardScaler:** Chosen to normalize numeric features to mean=0, std=1, which helps in model convergence (especially for distance-based models)."
      ],
      "metadata": {
        "id": "dXT-LAX7LMrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Is there a need for dimensionality reduction and reason."
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, since most features are either categorical or engineered, and feature count is already manageable (~20 columns)."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_features, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Splitting ratio used and reason"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80:20 used to ensure enough data for both training and validation; stratified to maintain CSAT score distribution."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Model - 1: Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "# Use the already defined X_train and y_train from the data splitting section\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used:"
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression was selected for baseline modeling because:\n",
        "\n",
        "*   It’s interpretable and works well on binary or multiclass classification problems.\n",
        "*   Fast training and gives us a performance benchmark.\n"
      ],
      "metadata": {
        "id": "4Vr6VCcSO7nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ML Model - 2: Random Forest Classifier"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and fit\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Random Forest?"
      ],
      "metadata": {
        "id": "TyFmt4k9Rcnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Handles missing values and categorical variables better.\n",
        "*   Ensemble method, reduces variance.\n",
        "\n",
        "*   Gives feature importance insights."
      ],
      "metadata": {
        "id": "uW8NXMPsROwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEST MODEL - LOGISTIC REGRESSION\n"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCLUSION"
      ],
      "metadata": {
        "id": "oci7IQiIYTEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully analyzed Flipkart's customer support dataset to identify key factors influencing customer satisfaction (CSAT). Through comprehensive data cleaning, exploratory data analysis, and statistical hypothesis testing, we gained valuable insights into customer support operations.\n",
        "\n",
        "The data wrangling process effectively handled missing values, outliers, and categorical variables, preparing the dataset for modeling. Text preprocessing on customer remarks provided additional textual features, although their impact on the final models was not explicitly evaluated as part of the reported results.\n",
        "\n",
        "Several machine learning models, including Logistic Regression and Random Forest, were implemented and evaluated. Based on the provided classification reports and accuracy scores, the Logistic Regression model demonstrated comparable or slightly better performance on the test set compared to the Random Forest model in this instance. This suggests that a simpler, more interpretable model like Logistic Regression can be effective for this dataset and problem.\n",
        "\n",
        "Key drivers of CSAT identified through the analysis and feature importance included category, Sub-category, Agent_name, and response_time_mins. These findings align with the business intuition that the nature of the issue, the agent handling it, and the speed of resolution are crucial to customer satisfaction.\n",
        "\n",
        "Hypothesis testing revealed statistically significant differences in CSAT scores across different support channels and potentially influenced by supervisors, highlighting areas for targeted improvement and training.\n",
        "\n",
        "Overall Business Impact:\n",
        "\n",
        "The insights gained from this project can empower Flipkart to make data-driven decisions to enhance customer support:\n",
        "\n",
        "Targeted Agent Training: Focus training on agents and supervisors whose teams exhibit lower CSAT scores, emphasizing best practices observed in high-performing teams.\n",
        "Process Optimization: Prioritize efforts to reduce response times and resolution times, particularly for high-volume and low-CSAT categories.\n",
        "Channel Strategy: Evaluate the performance of different support channels and consider optimizing resources and strategies based on their impact on CSAT.\n",
        "Proactive Issue Resolution: Address the root causes of common issue categories like \"order issues\" and \"return issues\" to reduce overall ticket volume and improve customer experience.\n",
        "While Logistic Regression served as a solid baseline and performed well, further exploration with other models and hyperparameter tuning could potentially yield even better results. However, the current findings provide actionable insights that Flipkart can leverage to improve customer satisfaction and potentially reduce churn."
      ],
      "metadata": {
        "id": "8Q5kaaDMYWvh"
      }
    }
  ]
}