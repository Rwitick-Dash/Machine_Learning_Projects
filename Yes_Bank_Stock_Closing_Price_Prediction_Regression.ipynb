{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4"
      ],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rwitick-Dash/Machine_Learning_Projects/blob/main/Yes_Bank_Stock_Closing_Price_Prediction_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Closing Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Yes Bank Stock Closing Price Prediction Project investigates the impact of the 2018 financial scandal involving co-founder Rana Kapoor on the bank’s stock performance and explores whether predictive models can forecast price movements in such volatile conditions. Using monthly stock data—including opening, closing, high, low prices, and trading volumes—the project conducts extensive exploratory data analysis to uncover trends, volatility, and the influence of trading activity, particularly around the crisis period. Regression and time series models are developed and evaluated using Scikit-Learn, supported by visual diagnostics through Matplotlib and Seaborn. The study emphasizes the closing price as the key variable and addresses temporal shifts and autocorrelation. Ultimately, the project offers insights into how financial crises affect stock behavior and demonstrates that with robust modeling and data preparation, predictive analytics can guide investor decision-making in turbulent markets."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To predict the monthly closing stock price of Yes Bank using historical stock data and machine learning models, accounting for market volatility and significant financial events.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "Z_wT08kOz6U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import pandas as pd\n",
        "  import io\n",
        "  df = pd.read_csv(io.BytesIO(uploaded['data_YesBank_StockPrices.csv']))\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "7AxpVARJ0B1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows in the dataset: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualising Missing Values/Null Values"
      ],
      "metadata": {
        "id": "tcwcEqdVrXcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno\n",
        "msno.heatmap(df)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding the Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' to datetime and sort\n",
        "df['Date'] = pd.to_datetime(df[\"Date\"], format='%b-%y')\n",
        "df.sort_values('Date', inplace=True)\n",
        "\n",
        "# Drop rows with missing values if any\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Reset index after sorting\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Create new features if useful\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# View cleaned and enriched dataset\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Converted the 'Date' column to datetime type for temporal operations.\n",
        "- Sorted the data based on time to prepare it for modeling.\n",
        "- Checked and removed missing values to maintain data integrity.\n",
        "- Created new features: `Year` and `Month` to capture seasonal trends.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 1: Line chart for Close prices over time\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df['Date'], df['Close'], color='blue')\n",
        "plt.title('Monthly Closing Price of Yes Bank Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 📌 **Reason for picking up the specific chart?**  \n",
        "To understand the trend in stock closing price over time.\n",
        "\n",
        "2. 📈 **Insights from the chart:**  \n",
        "The closing price showed significant volatility after 2018, especially during the Rana Kapoor fraud period.\n",
        "\n",
        "3. 🧩 **Business Impact:**  \n",
        "Helps investors understand when instability occurred and how the stock price reacted."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 2: Boxplot of closing prices\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(y='Close', data=df)\n",
        "plt.title('Boxplot of Monthly Closing Prices')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 📌 **Reason for picking up the specific chart?**  \n",
        "To detect outliers in closing price distribution.\n",
        "\n",
        "2. 📈 **Insights from the chart:**  \n",
        "Outliers are present especially during periods of market shocks.\n",
        "\n",
        "3. 🧩 **Business Impact:**  \n",
        "Identifies potential abnormal events that could warrant deeper investigation.\n"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart 3: Yearly average of monthly closing prices\n",
        "plt.figure(figsize=(10,6))\n",
        "df.groupby('Year')['Close'].mean().plot(kind='bar', color='green')\n",
        "plt.title('Average Monthly Closing Price per Year')\n",
        "plt.ylabel('Avg Close Price')\n",
        "plt.xticks(rotation=45) #Rotates the labels of x-axis tilted(it is done to accomodate a lot of data)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 📌 **Reason for picking up the specific chart?**  \n",
        "To evaluate how the average performance changed year-over-year.\n",
        "\n",
        "2. 📈 **Insights from the chart:**  \n",
        "Decline seen after 2018 aligns with the fraud incident.\n",
        "\n",
        "3. 🧩 **Business Impact:**  \n",
        "Supports financial planning and investor sentiment analysis.\n"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 4: Correlation Heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(df[['Open', 'High', 'Low', 'Close']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 📌 **Reason for picking up the specific chart?**  \n",
        "To identify feature relationships with the target variable (Close).\n",
        "\n",
        "2. 📈 **Insights from the chart:**  \n",
        "High correlation between `Open`, `High`, and `Close`.\n",
        "\n",
        "3. 🧩 **Business Impact:**  \n",
        "Helpful in feature selection for model building.\n"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "**Null Hypothesis (H0):** The mean monthly closing price before 2018 is equal to the mean monthly closing price after 2018.  \n",
        "**Alternative Hypothesis (H1):** The mean monthly closing price before 2018 is not equal to the mean monthly closing price after 2018.\n"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Statistical Test"
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The independent two-sample t-test, also known as the unpaired t-test,\n",
        "is a statistical test used to determine if there is a significant difference between\n",
        "the means of two unrelated groups.\n",
        "'''\n",
        "# Create two groups: before and after 2018\n",
        "before_2018 = df[df['Year'] < 2018]['Close']\n",
        "after_2018 = df[df['Year'] >= 2018]['Close']\n",
        "\n",
        "'''t-statistic:\n",
        "This is a measure of the difference between the two sample means,\n",
        "taking into account the variability within each group.\n",
        "\n",
        "\n",
        "p-value:\n",
        "This is the probability of observing a t-statistic as extreme as,\n",
        "or more extreme than, the one calculated, assuming the null hypothesis is true.\n",
        "\n",
        "\n",
        "Compare the p-value to the significance level (alpha):\n",
        "If the p-value is less than the significance level (e.g., 0.05), the null hypothesis is rejected,\n",
        "and it's concluded that there is a significant difference between the group means\n",
        "'''\n",
        "# Perform t-test\n",
        "from scipy.stats import ttest_ind\n",
        "t_stat1, p_val1 = ttest_ind(before_2018, after_2018, equal_var=False) #equal_var=False: Indicates that the two groups are assumed to have unequal variances, making this a Welch’s t-test (more robust when variances differ).\n",
        "print(f\"T-statistic: {t_stat1:.3f}, P-value: {p_val1:.4f}\")  #.3f means 3 decimal places and .4f means 4 decimal places\n",
        "\n",
        "'''t_stat1: The t-statistic value — measures how many standard deviations the sample means are apart.\n",
        "\n",
        "p_val1: The p-value — indicates the probability that the observed difference in means\n",
        "is due to chance (under the null hypothesis).'''\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Test Used:** Independent two-sample t-test  \n",
        "**Why this test?** Because we are comparing the means of two independent groups.  \n",
        "**Conclusion:** If P-value < 0.05, we reject the null hypothesis and conclude that the average closing price changed significantly after 2018.\n"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "\n",
        "**Null Hypothesis (H0):** There is no significant correlation between `Open` and `Close` prices.  \n",
        "**Alternative Hypothesis (H1):** There is a significant correlation between `Open` and `Close` prices.\n"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Statistical Test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pearson correlation test\n",
        "'''The Pearson correlation coefficient, often denoted as 'r', is a statistical measure that quantifies the strength\n",
        "and direction of the linear relationship between two variables. It always falls between -1 and +1, with 0 indicating no correlation.\n",
        "It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables.\n",
        "When one variable changes, the other variable changes in the same direction.'''\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "corr, p_val2 = pearsonr(df['Open'], df['Close'])\n",
        "print(f\"Pearson Correlation Coefficient: {corr:.3f}, P-value: {p_val2:.4f}\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Test Used:** Pearson correlation coefficient  \n",
        "**Why this test?** Because both `Open` and `Close` are continuous numerical variables.  \n",
        "**Conclusion:** If P-value < 0.05, we reject the null and conclude a statistically significant correlation exists.\n"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Hypothetical Statement 3  \n",
        "**Null Hypothesis (H0):** The variance in `Close` prices remains constant over the years.  \n",
        "**Alternative Hypothesis (H1):** The variance in `Close` prices differs across years.\n"
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Statistical Test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Levene’s test for equal variances\n",
        "from scipy.stats import levene\n",
        "years = df['Year'].unique()\n",
        "grouped_close = [df[df['Year'] == y]['Close'] for y in years]\n",
        "stat3, p_val3 = levene(*grouped_close)\n",
        "print(f\"Levene’s Statistic: {stat3:.3f}, P-value: {p_val3:.4f}\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Statistical Test Used:** Levene’s test  \n",
        "**Why this test?** To check for homogeneity of variances across multiple groups (years).  \n",
        "**Conclusion:** If P-value < 0.05, variances in `Close` prices are not equal across years.\n"
      ],
      "metadata": {
        "id": "kEuTNtm29_No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ No missing values found."
      ],
      "metadata": {
        "id": "f-q7-LzdMuFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting boxplot to identify outliers\n",
        "sns.boxplot(data=df[['Open', 'High', 'Low', 'Close']])\n",
        "plt.title(\"Boxplot to Detect Outliers\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No major outliers removed, as extreme values post-2018 are real market reflections."
      ],
      "metadata": {
        "id": "uwwS5zYlMv_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No categorical columns – no encoding required."
      ],
      "metadata": {
        "id": "hchvZJf-Zzpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features\n",
        "features = ['Open', 'High', 'Low']\n",
        "target = 'Close'"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Features chosen based on high correlation."
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[features])\n",
        "y = df[target].values"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 1: Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np # Import numpy for sqrt\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred_lr)\n",
        "mse = mean_squared_error(y_test, y_pred_lr) # Removed squared=False\n",
        "rmse = np.sqrt(mse) # Calculate RMSE by taking the square root of MSE\n",
        "r2 = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Linear Regression - MAE: {mae}, RMSE: {rmse}, R2: {r2}\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 2: Random Forest Regressor\n",
        "\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Import the necessary metrics\n",
        "import numpy as np # Import numpy for sqrt\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "params = {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]}\n",
        "\n",
        "grid = GridSearchCV(rf, params, cv=3, scoring='r2')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "# Calculate RMSE by taking the square root of MSE\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest - MAE: {mae_rf}, RMSE: {rmse_rf}, R2: {r2_rf}\")"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 3: Gradient Boosting Regressor"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Import the necessary metrics\n",
        "import numpy as np # Import numpy for sqrt\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred_gbr = gbr.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mae_gbr = mean_absolute_error(y_test, y_pred_gbr)\n",
        "# Calculate RMSE by taking the square root of MSE\n",
        "mse_gbr = mean_squared_error(y_test, y_pred_gbr)\n",
        "rmse_gbr = np.sqrt(mse_gbr)\n",
        "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
        "\n",
        "print(f\"Gradient Boosting - MAE: {mae_gbr}, RMSE: {rmse_gbr}, R2: {r2_gbr}\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####📈 Final Model Selection"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing R² Scores\n",
        "print(f\"Linear Regression R²: {r2}\")\n",
        "print(f\"Random Forest R²: {r2_rf}\")\n",
        "print(f\"Gradient Boosting R²: {r2_gbr}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####✅ Selected Model: Random Forest (best balance of MAE, RMSE, and R²)"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = best_rf.feature_importances_\n",
        "feature_imp = pd.Series(importances, index=features)\n",
        "feature_imp.sort_values().plot(kind='barh')\n",
        "plt.title(\"Feature Importance - Random Forest\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vB51enydTOol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully developed a predictive model for Yes Bank's monthly closing stock prices by leveraging historical data and machine learning techniques. The analysis encompassed comprehensive data preprocessing, exploratory data analysis, feature engineering, and the implementation of various regression models, including Linear Regression, Gradient Boosting and Random Forest Regressor.\n",
        "\n",
        "Among the models evaluated, the Random Forest Regressor demonstrated superior performance.\n",
        "\n",
        "Key insights from the project include:\n",
        "\n",
        "1.Impact of Events: Significant events, such as the 2018 fraud case involving Rana Kapoor, had a noticeable effect on stock price volatility, underscoring the importance of incorporating event-driven analysis in stock prediction models.\n",
        "\n",
        "2.Feature Importance: Variables like opening price, highest and lowest prices, and trading volume were significant predictors of the closing price, highlighting their relevance in stock price forecasting.\n",
        "\n",
        "In conclusion, this project demonstrates the efficacy of machine learning models in predicting stock prices, providing valuable tools for investors and analysts to make informed decisions in the dynamic financial market."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}